\section{Interoperability for Data Quality Screening}

A second aspect we want to explore is the possibility to combine data sources that are generated inside of the health system to monitor and improve the quality of data collected in Health Information Systems.


%% TODO document RBF program in Bénin. Who funds, when started, what is being funded.
This work is conducted in collaboration with the Belgian startup Bluesquare. Bluesquare has developed a data system for management and validation of Results Based Financing indicators (RBF), called OpenRBF.  In systems using OpenRBF, data is collected in facilities for indicators contracted in the RBF program, and are reported at district level on a monthly basis. The District administrators are in charge of entering the data in the OpenRBF database. This monthly data is then aggregated, and checked on a quarterly basis for quality. Data quality check is made through a field visit made by project managers in facilities, which will then check the quality of primary data collection in facilities (reports, charts) and the quality of reporting, by comparing collected primary data and reported numbers.

OpenRBF has been implemented in Bénin in XX facilities in YY départements since AAAA and in XX facilities in YY aires de santé in the DRC since AAAA.

This system allows to improve the confidence and exactitude of reported numbers, on which payments to facilities depend. Meanwhile, it is costly and does require regular field visits by program managers, in order to identify on average XX data problems per month, leading to savings of YY\$ by month.

There is a need to target data verification to make it more cost efficient. Having a tool that allows a screening of data quality in order to inform data verification strategies would be of primary use for programs using OpenRBF. We will design such a tool, exploring how we data quality can be measured at report level or at facility level. The outcome of his project will be a dashboard that will provide a simple metric or a combination of metrics to orient project managers in planning their data verification activities.

\subsection{Research questions}

The main research question of the project is : \textit{How to best measure data credibility in Health Facilties indicator reports}. This question will be exploring

\begin{description}
	\item[modeling] we will find the best modeling approach to infer the quality of the data collected in facilities and we will chose a favorite measure and sythetize data quality.
	\item[vizualisation] we will build a data vizualisation tool to allow program managers to efficiently explore data quality in their programs and spot data issues.
	\item[program management] we will derive program management routines to make best use of our work, that could be in later stages be integrated in the routine OpenRBF software distribution.
\end{description}

\subsection{Data}
\label{paper2_data}

Bénin's health system is using both OpenRBF and DHIS2 to store and manage data collected in health facilities. OpenRBF has been implemented and used nationwide since AAAA, and DHIS2 has been implemented and used nationwide since BBBB. As a result, as of DDDD, RRRR reports have been collected in OpenRBF in FFFF facilities, constituting GGGG facility months of data. We will also have HHHH facility months of data in DHIS2, and IIII facility months of data in common between the two data sources.

The data has been obtained, courtesy to Bluesquare and of the Bénin Ministry of Health.

\subsection{Method}

Our main aim will be to evaluate the credibility of a data value or a dataset and provide an action recommendation pertaining to the control of this data.  Additionally, we want to provide some evidence as of what the source of error or missingness in a dataset is. There are three main situations we want to be able to differentiate between :
\begin{description}
	\item[simple outlier] are situations when an isolated data value in a dataset is wrong, for one facility once. These are the situations that are the most commonly recognized as outliers.
	\item[outlying report] are situations when all values of one report appear to be off. This may be due to an update in the tools or methods for some indicators, or to the training of a new Health Worker in the facility who does not fully comply with usual ways to compute indicators. To identify this type of issue, it is necessary to compare
	\item[outlying facility] are situations when one facility is consistently reporting numbers that are different from surrounding facilities. This may happen when structural conditions in one facility are leading to discrepancies in data collection, or in data computation.
\end{description}

Our general approach will be to attach to each data value a probabilistic value for its credibility. The combination of these credibility measures for a dataset will give an overall estimation of the credibility of this dataset. In a third and last step, we will explore methods for imputation of data values with low credibility or for methods with low credibility.


%% ie ce qu'on propose n'est pas orienté pour corriger les données mais pour construire un meilleurs système par la suite. Plus orienté développement et value l'utilisation des données plus que leur perfection. Imputation pour données fausses.

\subsubsection{Variable Imputation}

To spot data values with a low chance of being right, we  will be to impute expected values for different indicators, and to measure the deviation of the actual value from the expected ones.

\paragraph{General Approach} For indicator $X_{i*}$ measured at time $t*$ in facility $f*$ in report type $r*$, we will evaluate an imputed distribution using variables from both report $r*$ and from other data sources.

$$ \widetilde{X}_{i^*,t^*,f^*,r^*} \sim \operatorname{D} \left(f\left(X_{i,t,f,r}\right)_{ \left(i,t,f,r\right) \neq \left(i^*,t^*,f*,r^*\right) } \right) $$

where $\operatorname{D}$ is an unspecified distribution derived from available data excluding the data point or the group of data points $\widetilde{X}_{i^*,t^*,f^*}$.

The form and parameters of $\operatorname{D}$ will be derived by fitting a wide variety of models, and dynamically selecting the best performing ones for the data considered.

To make the approach more efficient, we will keep a running count of best performing models, and we will fit already kept models first, and weight which models will be used in later  runs of the tool for similar data.

% TODO Document different models I will use
% TODO Document stacks vs bagging


\subsection{Credibility Pattern Screening}
\label{paper2_credib_pattern}

In order to guide and inform supervision, the tool we will design should be able to provide richer information than just rating of a particular data value's credibility. One should be able to provide information on wether detected error is due to single shot error, or if it is part of a recurrent error scheme, or if a particular facility reports consistently outlying results.

The validity probability of each data value will be pooled at report, facility and district levels. An analysis of the distribution of credibility at each level of aggregation will be made to characterize the type of data error pattern in one of four categories : No error, Single outliers, Outlying report or Outlying facility.
\begin{description}
	\item[No error] will be situations in which no data values will be over the fixed threshold of acceptable credibility.
	\item[Single Outliers] will be situations in which less than X\% of data values in a single report for a facility will be under the fixed threshold of acceptable credibility.
	\item[Outlying report] will be situations in which more than X\% of data values in a single report for a given facility will be under the fixed threshold of acceptable credibility.
	\item[Outlying facilities] will be situations in which more than X\% of data values will be under the fixed threshold of acceptable credibility in more than N consecutive reports for the same facility.
\end{description}

Finally, we will offer different ways to rank reports by or values by data quality status, depending on program priority and the risk they are most willing to accept:
\begin{description}
	\item[]
	\item[Quality risk] A program may want to identify with truly outlying numbers, which may be due to weak performance of the facility, or to specific local population or epidemiology characteristics.
	\item[Cost risk] A program may prioritize costs control, and want to explore suspect data that could bear a heavy cost. In this prioritizing mode, we will rank higher facilities for which the difference between communicated numbers and expected numbers would engender the highest costs for the RBF program.
\end{description}

\subsection{Outputs}

As a result of this project, we will deliver :
\begin{enumerate}
	\item A paper
	\item A data vizualisation
	\item A script for easy integration of our results in OpenRBF
	      \subsection{Timeline}

	      Figure \ref{Gantt2} will present a timeline for the realization of this objective.

	      \begin{figure}[h]
	      	\begin{ganttchart}[vgrid,hgrid]{1}{24}
	      		\gantttitle{2016}{12}
	      		\gantttitle{2017}{12} \\
	      		\gantttitlelist{1,...,12}{1} \gantttitlelist{1,...,12}{1}\\
	      		%\ganttgroup{Group 1}{1}{7} \\
	      		\ganttbar{Data Extraction}{1}{3} \\
	      		\ganttbar{Data Cleaning}{2}{4} \\
	      		\ganttbar{Data Analysis 1}{5}{6} \\
	      		\ganttmilestone{Sharing First Results}{6} \\
	      		\ganttbar{Data Analysis 2}{7}{9} \\
	      		\ganttmilestone{Sharing Final Results}{9} \\
	      		\ganttbar{Paper Writing}{9}{11} \\
	      		\ganttmilestone{Paper Submission}{11}
	      	\end{ganttchart}
	      	\caption{Gantt Chart for Paper 2}
	      	\label{Gantt2}
	      \end{figure}
