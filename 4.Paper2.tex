\section{Interoperability for Data Quality Screening}

A second aspect we want to explore is the possibility to combine data sources that are generated inside of the health system to monitor and improve the quality of data collected in Health Information Systems.

This work is conducted in collaboration with the Belgian startup Bluesquare. Bluesquare has developed a data system for management and validation of Results Based Financing indicators (RBF), called OpenrRBF.  In this framework, data collected in facilities for RBF program are reported and integrated on a monthly basis in OpenRBF. This monthly data is then aggregated, and checked on a quarterly basis for quality. Data quality check is made through a field visit made by project managers in facilities, which will then check the quality of primary data collection in facilities (reports, charts) and the quality of reporting, by comparing collected data and reported numbers.

OpenRBF has been implemented in Bénin in XX facilities in YY départements since AAAA and in XX facilities in YY aires de santé in the DRC since AAAA.

This system allows to improve the confidence of reported numbers, on which payments to facilities depend. Meanwhile, it is costly and does require regular field visits by program managers, in order to identify on average XX data problems per month, leading to savings of YY\$ by month.

In a programmatic sense, there is a need to target data verification to make it more cost efficient. Having a tool that allows a screening of data quality in order to inform data verification strategies would be of primary use for programs using OpenRBF. We will design such a tool, exploring how we data quality can be measured at report level or at facility level. The outcome of his project will be a dashboard that will provide a simple metric or a combination of metrics to orient project managers in the verification of data.

\subsection{Research questions}

I will thus explore methods to validate, correct and complete data sets from routine HMIS, and measure the benefits of indicators matching between different dataset to improve validation, correction and completion. This will be considered looking at different intermediary questions :

\begin{enumerate}
	\item What is a good metric to assert data quality and completeness of a given HMIS data set ?
	\item What is the performance of different approaches to HMIS data validation, correction and completion ?
	\item What is the benefit of using mapped datasets on this last metric ?
\end{enumerate}

\subsection{Data}
\label{paper2_data}

Bénin has been implementing and using DHIS2 nationwide since AAAA, and DRC has been rolling it out since AAAA. Similar information is collected in both DHIS2 and OpenRBF, especially when it comes to XXXX.


\subsection{Method}

Our main aim will be to gauge the credibility of a data value or a dataset.  Additionally, we want to provide some evidence as of what the source of error or missingness in a dataset is. There are three main situations we want to be able to differentiate between :
\begin{description}
	\item[simple outlier] are situations when an isolated data value in a dataset is wrong, for one facility once. These are the situations that are the most commonly recognized as outliers.
	\item[outlying report] are situations when all values of one report appear to be off. This may be due to an update in the tools or methods for some indicators, or to the training of a new Health Worker in the facility who does not fully comply with usual ways to compute indicators. To identify this type of issue, it is necessary to compare
	\item[outlying facility] are situations when one facility is consistently reporting numbers that are different from surrounding facilities. This may happen when structural conditions in one facility are leading to discrepancies in data collection, or in data computation.
\end{description}

Our general approach will be to attach to each data value a probabilistic value for its credibility. The combination of these credibility measures for a dataset will give an overall estimation of the credibility of this dataset, and the type of error it may suffer from. In a third and last step, we will explore methods for imputation of data values with low credibility or for methods with low credibility.


%% ie ce qu'on propose n'est pas orienté pour corriger les données mais pour construire un meilleurs système par la suite. Plus orienté développement et value l'utilisation des données plus que leur perfection. Imputation pour données fausses.

We will test two approaches to do so :

\begin{description}
	\item[Error prediction] Using the validation dataset from OpenRBF, we will try and predict wrong data values using a simple predictive approach and bagging different Machine Learning Classification methods. Result of this approach will be a probabilistic assessment of data quality for each indicator value.
	\item[Variable imputation] Using all available data, we will impute all data value and get a posterior estimated distribution of the value.
\end{description}

\subsubsection{Error Prediction}

For a each indicator value, we will model the probability that the value is right. We will use a logistic model specified as :
%% TODO specification logistic model

Using \textit{Forward model selection} and training models on data on verified values (cf. \ref{paper2_data}), we will find the best models to predict data value errors. We will then use this model out of sample to compute the probability a value is true on each value.

\subsubsection{Variable Imputation}

For a given indicator $X_{i*}$ measured at time $t*$ in facility $f*$, we will compute an imputed distribution based on all other information available in our data. A general representation of this approach could be written as:

$$ \widetilde{X}_{i^*,t^*,f^*} \sim \operatorname{D} \left(f\left(X_{i,t,f}\right)_{ \left(i,t,f\right) \neq \left(i^*,t^*,f*\right) } \right) $$

where $\operatorname{D}$ is an unspecified distribution derived from a function of all available data excluding the data point $\widetilde{X}_{i^*,t^*,f^*}$.

Using verified data as our validation set, we will define a threshold for credibility of data that we will later use as a decision rule for considering data as regular or outlying (see \ref{paper2_credib_pattern}).

We will test different approaches for this model.
% TODO Document different models I will use


\subsection{Credibility Pattern Screening}
\label{paper2_credib_pattern}

The validity probability of each data value will be pooled at report, facility and district levels. An analysis of the distribution of credibility at each level of aggregation will be made to characterize the type of data error pattern in one of four categories : No error, Single outliers, Outlying report or Outlying facility.
\begin{description}
	\item[No error] will be situations in which no data values will be over the fixed threshold of acceptable credibility.
	\item[Single Outliers] will be situations in which less than X\% of data values in a single report for a facility will be under the fixed threshold of acceptable credibility.
	\item[Outlying report] will be situations in which more than X\% of data values in a single report for a given facility will be under the fixed threshold of acceptable credibility.
	\item[Outlying facilities] will be situations in which more than X\% of data values will be under the fixed threshold of acceptable credibility in more than N consecutive reports for the same facility.
\end{description}

\subsection{Performance Metric}

Validation of these methods will be made using cross-validation. We will first evaluate the data validation framework independently of the indicators mapping framework. We will then test the performance of the combination of different indicator mapping methods and data validation methods.


%\subsection{Semantic interoperability framework}

%We will first test different approaches to semantic interoperability. This project is building on an existing project in which we have been defining and testing a structure tagging approach to interoperability. With this approach, each health service indicator in a given facility reporting framework is defined based on three dimensions : health issue, population target and type of service provided. A first level of testing has already implemented in Bénin, matching DHIS2 indicators with OpenRBF indicators. Using this simple method, it was possible to match correctly 15 of 21 indicators from the OpenRBF framework with a subset of 500 indicators from the DHIS2 framework.

%One of the biggest benefits of this approach is its ability to define a notion of \textit{distance} between indicators. Indicators that are related by one dimension will be farther apart from each other than indicators that are related by two dimensions. Also, for dimensions that are not defined on a discrete basis (for example age boundaries for a population), incomplete correspondence can be quantified. This notion of distance can in a later stage be used as a weight in comparison of different indicators. In the currently tested version, this distance is linearly additive, but other formulation of this distance can be tested.

%We will also test alternative approaches, such as Latent Semantic Indexing, and completely blind indicators matching based on values and trends.


\subsection{Timeline}

Figure \ref{Gantt2} will present a timeline for the realization of this objective.

\begin{figure}[h]
	\begin{ganttchart}[vgrid,hgrid]{1}{24}
		\gantttitle{2016}{12}
		\gantttitle{2017}{12} \\
		\gantttitlelist{1,...,12}{1} \gantttitlelist{1,...,12}{1}\\
		%\ganttgroup{Group 1}{1}{7} \\
		\ganttbar{Data Extraction}{1}{3} \\
		\ganttbar{Data Cleaning}{2}{4} \\
		\ganttbar{Data Analysis 1}{5}{6} \\
		\ganttmilestone{Sharing First Results}{6} \\
		\ganttbar{Data Analysis 2}{7}{9} \\
		\ganttmilestone{Sharing Final Results}{9} \\
		\ganttbar{Paper Writing}{9}{11} \\
		\ganttmilestone{Paper Submission}{11}
	\end{ganttchart}
	\caption{Gantt Chart for Paper 2}
	\label{Gantt2}
\end{figure}
