\section*[Data Monitoring]{Aim 3 - Health System Data Monitoring and Screening}

The second project will be aimed at exploring methods for screening and monitoring of data collected in health systems to inform and orient supervisions. Specifically, we will consider data collected in Result Based Financing programs. Results Based Financing (RBF) is a mode of financing of health systems based on the ex-post payment of health facilities by a national financing body or program. This mode of financing alleviates the burden of planning for coordinating bodies, but transfers it to increased reporting needs for facilities. In order to receive their payment, facilities have to report monthly on a set of indicators on which this payment will be based.

In a RBF system, understanding and monitoring results is thus important on two main levels. First, Program managers want to have an accurate evaluation of the activity they should be paying for, as the accurate evaluation of this amount is key to the sustainability of the program at local and aggregate level. Underfinancing facilities endangers their ability to operate in good conditions. Meanwhile overpaying some facilities may be detrimental to the overall sustainability of the project. Second, being able to measure and compare facilities performance is essential to identify weakly performing facilities, and to start implementing correcting measures.

In Bénin, the Programme de Renforcement du Système de Santé (PRSS) has been launched in 2011 by a consortium formed by the Beninese government, the World Bank, the GAVI Alliance and the Global Fund. As part of this program, a comprehensive RBF program has been implemented in all 34 health zones of the country. To allow the management of RBF reporting data, the software OpenRBF, developed by the Belgian startup Bluesquare has been implemented.

In systems using OpenRBF, data is collected in facilities for indicators contracted in the RBF program, and are reported at district level on a monthly basis. The District administrators are in charge of entering the data in the OpenRBF database. This monthly data is then aggregated, and checked on a quarterly basis for quality. Data quality check is made through a field visit made by project managers in facilities, which will then check the quality of primary data collection in facilities (reports, charts) and the quality of reporting, by comparing collected primary data and reported numbers.

This system allows to improve the confidence and exactitude of reported numbers, on which payments to facilities depend. Meanwhile, it is costly and does require regular field visits by program managers, in order to identify on average XX data problems per month, leading to savings of YY\$ by month.

This system has proved its worth in allowing RBF program managers to access credible data on facilities performance. Meanwhile, in a program managing close to 800 facilities, it is increasingly unceasingly difficult to validate data, and to monitor service quality in facilities. We will develop an approach to screen indicators reported by facilities, to help program managers making decisions. The decision framework in which we operate is quite simple, and has two main outcomes : validate data or not, raise a service quality issue or not. These two outcomes are not independent, and all depend on an evaluation of the normality of reported data when compared to previously observed data.

\subsection{Research questions}

This project will be aimed at developing and validating a generic framework for screening and validating data reported by facilities in the Bénin RBF program. We will develop this framework in four specific objectives.

\begin{description}
	\item[Data screening] In a first step, imputed distributions will be compared to reported data. We will develop and validate a set of rules and visualization to guide and inform decisions makers to validate data, and to judge quality of services based on reported data.
	\item[Classification]
	\item[Program Management] Finally, we will simulate different strategies to prospectively collect data in facilities, so as to allow an optimal performance of imputation models at a minimum cost.
\end{description}

\subsection{Data}
\label{paper2_data}

OpenRBF has been implemented in Bénin since March 2012, and has been rolled out in every departements of the country. Data is collected and validated monthly for both the \textit{Minimum Activities Package} (PMA). As of January 2017, the data consists in 16131 facilty/months of PMA data in 671 different facilities. In each report, a median of 75\% of indicators are correctly reported.

\begin{center}
\begin{figure}[ht]
\includegraphics[width=\textwidth]{figure/correct_dist.pdf}
\caption{Distribution of the percentage of correct indicators by report}
\end{figure}
\end{center}

\subsection{Methods}
\label{sec:models}

Each month, each facility sends a report, composed of indicators. Each indicator $I$ has a unitary cost $C$. Thus, on month $t$, facility $f$ will receive a payment of the amount :
$$P_{t,f} = \sum_i I_{i,t,f} C_{i,t,f}$$

When taking into consideration a monthly report from a facility, we want to measure the marginal and joint probabilities of two events $Q$ and $D$, where :
\begin{enumerate}
	\item $Q = 1$ if the quality of care in the facility appears in accordance with the quality in other facilities.
	\item $D = 1$ if the data appears of sufficient quality to base payment of the facility on it.
\end{enumerate}

We are thus most interested in three distinct joint events :
\begin{itemize}
	\item $D = 0$ the data appears to be of insufficient quality to draw conclusions
	\item $D = 1 \cap Q = 0$ the data appears sufficient but quality of service seems poor
	\item $D = 1 \cap Q = 1$ the data appears sufficient and quality of service seems good
\end{itemize}


The definition of $D$ results from a budgetary arbitrage. A report is considered of insufficient quality if the expected monetary value of the payment made to facility based on this report is too severely affected by data quality. We are then interested in the difference between the payment based on reported data ($\bar{P}$) and the expected payment based on all validated information available ($P^{*}$).


I will approach this problem in two steps. I will first test multiple approaches for process monitoring, to detect and spot unlikely data patterns. I will then define a approach to classify detected abnormalities as either $D = 0$ or $D = 1 \cap Q = 0$.

\subsubsection{Data Screening}

Methods for continuous data inspection and screening come from the industrial statistics domain and are increasingly applied for healthcare monitoring \cite{woodall_use_2006,woodall_current_2014}. Spiegelhalter et al. provided et nice overview of how these tools can be used for health systems, for three main functions of interest : rating, screening and surveillance \cite{spiegelhalter_statistical_2012}. These methods are based on the standardization of data distributions, and the analysis of observed data to detect values diverging from their expected distributions. This second stage, geared toward hypothesis testing, can be made using visual tools based on adaptation of classical charts like Shewart charts, Cumulative Sum control charts (CUSUM) or Exponentially Weighted Moving Average charts (EWMA). I will compare and combine three approaches to this data screening question.

\paragraph{Syndromic Surveillance} A first approach is to define the expected distribution of a given series based on its past values. This approach is widely used in syndromic surveillance, and builds on a model-based CUSUM approach, where the expected count of cases for a given period is modeled based on historical data, and the CUSUM is updated at each period using the forecasting errors \cite{fricker_comparing_2008}. Fricker writes a general version of model-based CUSUM as monitoring $S(t)$ where :
$$ S(t) = \max(0 , S(t-1) + x(t) - k)$$

where $x(t) = \frac{Y(t) - \hat(Y)(t)}{\sigma_\epsilon}$ is a standardized prediction error for $Y(t)$ on $\sigma_\epsilon$ the variation of the model error, and $k$ is a tuning parameter that will have to be optimized to obtain detection characteristics for our model.

A reference implementation of this approach was developed by Farrington et al. \cite{farrington_statistical_1996} and updated by Noufaily et al. \cite{noufaily_improved_2013}, and is implemented in $R$. I will implement this approach to test its ability to detect and differentiate between data quality issues and regime changes for certain indicators series. I may also use the Z-Score aggregation approach described in \cite{bardsley_using_2009} to aggregate multiple indicators result for a given facility month.

\paragraph{Hierarchical Modeling} A second approach defines expected values from a facility based on a hierarchical model including all facilities. Using bayesian hypothesis testing strategies based on multiple sampling from the posterior distribution, this approach allows me to spot unlikely numbers for a given facility as well as facility level unusual performances for a given indicator\cite{ohlssen_hierarchical_2007}. A hierarchical Poisson model for each $I_i$, in a facility $f$ can be defined as :
$$ I_{i,t^{*},f} \sim \mathrm{Poisson}(\exp{(\alpha_f + \sum_{i, t < t^{*}}{\beta_{i} I^v_{i,t,f}})})$$
$$\alpha_f \sim \mathrm{N}(0 , \sigma^2_\alpha)$$
$$ \forall (i , t , f) , \beta_{i,t,f} \sim \mathrm{N}(0 , \sigma^2_{\beta_{i,t,f}}) $$
For time $t^{*}$, $\tilde{I}_{i,t^{*},f}$ will be simulated by first simulating parameters $\alpha_f$ and $\beta_i$ and second drawing from the corrsponding Poisson distribution using the different parameters.

\paragraph{Profile Monitoring} Finally, we want to be able to detect pattern variations in reports. Indeed, the methods we described until now are targeted at monitoring univariate series for which desirable variations are clearly identified. In in-control situations, number of cases of monitored diseases are expected to stay low, and mortality rates are supposed to stay low. In the monitoring of RBF data, meanwhile, there is no clear denominator to construct rates, and there is little sense of what an "in control" situation should look like for individual indicators. Profile  monitoring is an increasingly popular research area\cite{woodall_current_2014}, and has many trends and applications, but I couldn't find any application in the healthcare sector. Its main idea is to monitor how indicators in the monitored process are related by a functional relationship, instead of monitoring individual indicators themselves. This relationship can be specified in parametric\cite{mahmoud_simple_2011} or non parametric ways\cite{chicken_nonparametric_2011}, and I will explore the options offered by the latter approach for the type of data we have.

\medskip

For each of these approaches, I will need to define thresholds to define unlikely data. This threshold will be a combination of analysis of the deviation of the observed indicators from the expected distributions, and will also incorporate an element of financial risk for. We noted the constraints of the RBF, which should aim at spending their budget efficiently, but ensure facilities have sufficient budget to operate their basic activities. I thus define the threshold of data credibility from a financial perspective as :

\begin{itemize}
\item For the upper limit, we want to identify the risk that the cost of bad data for the system exceeds a certain threshold. This can be defined as
$$ \bar{P} - P^{*} > \eta_1 \Rightarrow D = 0$$
with $\eta_1 > 0 $.

\item For the lower limit, we consider the risk of underfunding the facilities, if the payment claimed is significantly smaller than the expected payment. This can be defined as
$$  \frac{\bar{P}}{P^{*}} > \eta_2 \Rightarrow D = 0$$
with $0 < \eta_2 < 1 $.
\end{itemize}

For any given pair of thresholds ($\eta_1 , \eta_2$), for each report computed in month $t$ for facility $f$, I am thus interested in evaluating :
	$$p(D=0) = p(\bar{P} - P^{*} > \eta_1 \cup \frac{P^{*}}{\bar{P}} > \eta_2 ) $$
	$$p(D=0) = 1 - p(\bar{P} - P^{*} < \eta	_1 \cap \frac{P^{*}}{\bar{P}} < \eta_2 )$$
This will be made by sampling the posterior distributions of each $I_{i,f}$ and applying the correct tariff.

For any given month $t^{*}$ in the data, I will sequentially evaluate each approach in this section using validated data from previous periods $t < t^{*}$. I will be able to evaluate the ability of each approach to identify low quality data, or data that is too different from expected its values.

\subsubsection{Risk Classification}

The second step of this work will be to describe the risk associated with the reported data. As described earlier, I want to provide an indication of two different types of risk : a data quality risk ($D$) and a quality of care risk ($Q$). For any indicator or report spotted as behaving in an unexpected way, I will want to understand if the problem detected can be linked to data quality issues or to a modification of the conditions of care in the facility.

This classification will be made using the results of each methods used for data screening and combinations of their results as features for classification methods. Using the confirmation data, I will be able to train various supervised learning approaches for classifying data as diverging from the expected distributions.

\subsubsection{Validation and Operational fine tuning}

Performing algorithmic verification comes with a risk of creating reinforcing errors. Since we fit the expected distribution of $I_i$ using only the data that was previously validated on the field, the periodic creation of such validation data points for every facilities is essential to ensure these expected distributions are still in line with reality. Meanwhile, in facilities that demonstrate good measured data quality, data will be verified less frequently than previously. This holds potential risks for our approach to be able to detect variations in performance. The question we know ask is thus, what is the minimum of validated data that has to be available in a facility to allow algorithmic validation to perform well ?

The question at stake here is to find a best performing arbitrage between data verification costs and benefits of verification. To identify a best performing data validation strategy, we will compare four strategies :

\begin{enumerate}
	\item \textbf{Reactive validation only :} On site data validation is only performed in facilities were data quality issues have been suspected algorithmically.
	\item \textbf{Reactive and yearly validation :} On site data validation is performed in facilities in which data issues have been suspected, but every facility has to be visited at least once per year.
	\item \textbf{Reactive and bi-annual validation :} Same as the previous scenario, but every facility has to be visited at least once per semester.
	\item \textbf{Reactive and quarterly validation :} Same as the previous scenario, but every facility has to be visited at least once per quarter.
\end{enumerate}

The comparison will be made by simulating progressively the implementation of each strategy on the available data. We will be describing the performances of each approach in the same way we described the performance of the full model in section \ref{sec:models}.

\subsection{Timeline}
\label{timeline:aim2}

The data for this aim is already available and has been partially cleaned. I envision needing four months to implement the different data monitoring approaches, and four months for risk classification and operational fine tuning.

\begin{figure}[t]
	\begin{ganttchart}[vgrid,hgrid,
	y unit chart=.6cm]{1}{18}
		\gantttitle{2017}{12}
		\gantttitle{2018}{6} \\
		\gantttitlelist{1,...,12}{1} \gantttitlelist{1,...,6}{1}\\

		\ganttset{bar/.append style={draw=blue!40 , fill=blue!40},
                    group/.append style={draw=blue, fill=blue}}
        \ganttbar{Data Extraction}{1}{1} \\
		\ganttbar{Data Screening}{2}{5} \\
		\ganttmilestone{Sharing Data Screening results}{5} \\
		\ganttbar{Risk Classification}{6}{7} \\
		\ganttmilestone{Risk classification results}{7} \\
		\ganttbar{Operational Fine tuning}{8}{9} \\
		\ganttmilestone{Sharing Final Results}{9} \\
		\ganttbar{Paper Writing}{10}{11} \\
		\ganttmilestone{Paper Submission}{11}
	\end{ganttchart}
	\caption{Gantt Chart for Aim 2}
	\label{Gantt2}
\end{figure}
